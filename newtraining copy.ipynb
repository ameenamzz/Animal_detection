{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82415,
     "status": "ok",
     "timestamp": 1742540383546,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "kv6maIan5W1d",
    "outputId": "d9c81341-bf39-406e-ff74-072d71f87173"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics roboflow -q\n",
    "\n",
    "# Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12009,
     "status": "ok",
     "timestamp": 1742540395564,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "AuS-WR8y5W1g",
    "outputId": "d9cbe1a0-8e92-477f-90af-e5fcf68bf7b0"
   },
   "outputs": [],
   "source": [
    "# Your Roboflow API key\n",
    "api_key = \"\"\n",
    "\n",
    "# Function to download object detection dataset from Roboflow\n",
    "def download_detection_dataset(api_key, workspace, project_name, version, format_type=\"yolov8\", location=\"./datasets\"):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(format_type, location=f\"{location}/{project_name}\")\n",
    "    return dataset, f\"{location}/{project_name}\"\n",
    "\n",
    "# Function to download classification dataset from Roboflow\n",
    "def download_classification_dataset(api_key, workspace, project_name, version, location=\"./datasets\"):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(\"folder\", location=f\"{location}/{project_name}\")\n",
    "    return dataset, f\"{location}/{project_name}\"\n",
    "\n",
    "# Download the classification dataset\n",
    "print(\"Downloading Footprint Classification dataset...\")\n",
    "footprint_dataset, footprint_path = download_classification_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"sml-project-hfi0w\",\n",
    "    project_name=\"footprint-classification\",\n",
    "    version=1,\n",
    "    location=\"./footprint_dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1742540395644,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "dL434f755W1h",
    "outputId": "09fe9daa-c4ce-4bec-ac4c-45bf0a72b777"
   },
   "outputs": [],
   "source": [
    "# Identify classes in the footprint classification dataset\n",
    "footprint_classes = []\n",
    "for class_dir in os.listdir(f\"{footprint_path}/train\"):\n",
    "    if os.path.isdir(f\"{footprint_path}/train/{class_dir}\"):\n",
    "        footprint_classes.append(class_dir)\n",
    "\n",
    "print(f\"Footprint classification classes: {footprint_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17150,
     "status": "ok",
     "timestamp": 1742540412795,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "tnZl8eD05W1i",
    "outputId": "4c1ebef4-d536-46de-f13d-277f32bfaf9c"
   },
   "outputs": [],
   "source": [
    "# Download all other detection datasets\n",
    "print(\"Downloading object detection datasets...\")\n",
    "\n",
    "# Dataset 2: Wild Animal Foot Prints\n",
    "dataset2, dataset2_path = download_detection_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"traffic-bfp0e\",\n",
    "    project_name=\"wild-animal-foot-prints\",\n",
    "    version=1,\n",
    "    location=\"./dataset2\"\n",
    ")\n",
    "\n",
    "# Dataset 3: Fred's Test\n",
    "dataset3, dataset3_path = download_detection_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"fredstest\",\n",
    "    project_name=\"othsgdsreq\",\n",
    "    version=38,\n",
    "    location=\"./dataset3\"\n",
    ")\n",
    "\n",
    "# Dataset 4: New Animal Detection\n",
    "dataset4, dataset4_path = download_detection_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"lavhini\",\n",
    "    project_name=\"new_animaldetection\",\n",
    "    version=1,\n",
    "    location=\"./dataset4\"\n",
    ")\n",
    "\n",
    "print(\"All datasets downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1742540412841,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "q_CjR1S75W1i",
    "outputId": "f896094f-c837-4e37-dced-7edb1157713c"
   },
   "outputs": [],
   "source": [
    "# Load class information from detection datasets\n",
    "def get_classes_from_yaml(yaml_path):\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data.get('names', [])\n",
    "\n",
    "dataset2_classes = get_classes_from_yaml(f\"{dataset2_path}/data.yaml\")\n",
    "dataset3_classes = get_classes_from_yaml(f\"{dataset3_path}/data.yaml\")\n",
    "dataset4_classes = get_classes_from_yaml(f\"{dataset4_path}/data.yaml\")\n",
    "\n",
    "print(f\"Dataset2 classes: {dataset2_classes}\")\n",
    "print(f\"Dataset3 classes: {dataset3_classes}\")\n",
    "print(f\"Dataset4 classes: {dataset4_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1742540412892,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "dpEWsmWR5W1i",
    "outputId": "b3e97b0d-35ea-4e04-b8aa-80e20251058e"
   },
   "outputs": [],
   "source": [
    "# Create directory structure for the combined dataset\n",
    "combined_dataset_path = \"./combined_dataset\"\n",
    "os.makedirs(combined_dataset_path, exist_ok=True)\n",
    "\n",
    "# Create train, val, test directories\n",
    "for dir_name in [\"train\", \"valid\", \"test\"]:\n",
    "    # Create images and labels directories in each split\n",
    "    for sub_dir in [\"images\", \"labels\"]:\n",
    "        os.makedirs(f\"{combined_dataset_path}/{dir_name}/{sub_dir}\", exist_ok=True)\n",
    "\n",
    "# Create a unified class list\n",
    "unified_classes = footprint_classes + dataset2_classes + dataset3_classes + dataset4_classes\n",
    "\n",
    "# Remove any duplicate class names\n",
    "unified_classes = list(dict.fromkeys(unified_classes))\n",
    "print(f\"Unified classes ({len(unified_classes)}): {unified_classes}\")\n",
    "\n",
    "# Create a mapping for class indices across different datasets\n",
    "class_mappings = {\n",
    "    \"footprint\": {i: unified_classes.index(footprint_classes[i]) for i in range(len(footprint_classes))},\n",
    "    \"dataset2\": {i: unified_classes.index(dataset2_classes[i]) for i in range(len(dataset2_classes))},\n",
    "    \"dataset3\": {i: unified_classes.index(dataset3_classes[i]) for i in range(len(dataset3_classes))},\n",
    "    \"dataset4\": {i: unified_classes.index(dataset4_classes[i]) for i in range(len(dataset4_classes))}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VdPEJKb5W1j"
   },
   "outputs": [],
   "source": [
    "# Function to convert classification images to object detection format\n",
    "def convert_classification_to_detection(image_path, label_id, output_img_path, output_label_path):\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return False\n",
    "\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # Save image to output path\n",
    "    cv2.imwrite(output_img_path, img)\n",
    "\n",
    "    # Create detection annotation (full image as bounding box)\n",
    "    # Format: class_id x_center y_center width height\n",
    "    # All normalized to [0, 1]\n",
    "    x_center, y_center = 0.5, 0.5  # Center of image\n",
    "    w, h = 1.0, 1.0  # Full image\n",
    "\n",
    "    # Write label file\n",
    "    with open(output_label_path, 'w') as f:\n",
    "        f.write(f\"{label_id} {x_center} {y_center} {w} {h}\\n\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmH3uyTO5W1j"
   },
   "outputs": [],
   "source": [
    "# Function to process and remap label files for detection datasets\n",
    "def process_detection_label_file(src_file, dst_file, mapping):\n",
    "    with open(src_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    updated_lines = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 5:  # Ensure proper format: class_id x y w h\n",
    "            class_id = int(parts[0])\n",
    "            if class_id in mapping:\n",
    "                parts[0] = str(mapping[class_id])\n",
    "                updated_lines.append(' '.join(parts) + '\\n')\n",
    "\n",
    "    with open(dst_file, 'w') as f:\n",
    "        f.writelines(updated_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FIAqemG5W1l"
   },
   "outputs": [],
   "source": [
    "# Process Footprint Classification dataset (convert to detection format)\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    split_dir = \"train\" if split == \"train\" else \"valid\" if split == \"valid\" else \"test\"\n",
    "    if not os.path.exists(f\"{footprint_path}/{split_dir}\"):\n",
    "        print(f\"Split {split_dir} not found in footprint dataset\")\n",
    "        continue\n",
    "\n",
    "    # For each class folder\n",
    "    for class_idx, class_name in enumerate(footprint_classes):\n",
    "        class_dir = f\"{footprint_path}/{split_dir}/{class_name}\"\n",
    "        if not os.path.exists(class_dir):\n",
    "            continue\n",
    "\n",
    "        # Process each image in the class\n",
    "        for img_file in glob.glob(f\"{class_dir}/*.jpg\") + glob.glob(f\"{class_dir}/*.jpeg\") + glob.glob(f\"{class_dir}/*.png\"):\n",
    "            img_filename = f\"footprint_{class_name}_{os.path.basename(img_file)}\"\n",
    "            out_img_path = f\"{combined_dataset_path}/{split}/images/{img_filename}\"\n",
    "            out_label_path = f\"{combined_dataset_path}/{split}/labels/{os.path.splitext(img_filename)[0]}.txt\"\n",
    "\n",
    "            # Convert to detection format and save\n",
    "            mapped_class_id = class_mappings[\"footprint\"][class_idx]\n",
    "            convert_classification_to_detection(img_file, mapped_class_id, out_img_path, out_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1742540426929,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "NX3oPXza5W1l",
    "outputId": "418aa012-d74e-4801-fa5b-c3bec7c63303"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Process detection datasets\n",
    "dataset_paths = {\n",
    "    \"dataset2\": dataset2_path,\n",
    "    \"dataset3\": dataset3_path,\n",
    "    \"dataset4\": dataset4_path\n",
    "}\n",
    "\n",
    "# Function to combine detection datasets\n",
    "def combine_detection_datasets(dataset_path, dataset_key, split, class_mapping):\n",
    "    images_path = f\"{dataset_path}/{split}/images\"\n",
    "    labels_path = f\"{dataset_path}/{split}/labels\"\n",
    "\n",
    "    if not os.path.exists(images_path) or not os.path.exists(labels_path):\n",
    "        print(f\"Skipping {dataset_key} {split} - directory not found\")\n",
    "        return\n",
    "\n",
    "    # Process images\n",
    "    for img_file in glob.glob(f\"{images_path}/*\"):\n",
    "        img_filename = os.path.basename(img_file)\n",
    "        # Add dataset prefix to avoid filename conflicts\n",
    "        new_img_filename = f\"{dataset_key}_{img_filename}\"\n",
    "        shutil.copy(img_file, f\"{combined_dataset_path}/{split}/images/{new_img_filename}\")\n",
    "\n",
    "        # Process corresponding label if it exists\n",
    "        label_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
    "        label_file = f\"{labels_path}/{label_filename}\"\n",
    "        if os.path.exists(label_file):\n",
    "            new_label_file = f\"{combined_dataset_path}/{split}/labels/{dataset_key}_{label_filename}\"\n",
    "            # Remap class IDs according to our unified class list\n",
    "            process_detection_label_file(label_file, new_label_file, class_mapping)\n",
    "\n",
    "# Combine all detection datasets\n",
    "for dataset_key, dataset_path in dataset_paths.items():\n",
    "    print(f\"Processing {dataset_key}...\")\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        combine_detection_datasets(dataset_path, dataset_key, split, class_mappings[dataset_key])\n",
    "\n",
    "print(\"Datasets successfully combined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1742540426962,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "AZ6__wf85W1l",
    "outputId": "b285c66d-4669-4804-abf7-da410eabb4a1"
   },
   "outputs": [],
   "source": [
    "# Create YAML configuration file for the combined dataset\n",
    "yaml_content = {\n",
    "    'path': combined_dataset_path,\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': len(unified_classes),\n",
    "    'names': unified_classes\n",
    "}\n",
    "\n",
    "# Write YAML file\n",
    "yaml_path = f\"{combined_dataset_path}/data.yaml\"\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "print(f\"Created data.yaml with {len(unified_classes)} classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2015,
     "status": "ok",
     "timestamp": 1742540429059,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "Y3Z62eARffGS",
    "outputId": "afec09da-d1f0-46c5-fa65-cd6af1313c3e"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 55408,
     "status": "error",
     "timestamp": 1742540484481,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "0TyjhCjb5W1m",
    "outputId": "c38e1e38-ad4b-45e3-c45c-47d3eb7f66d5"
   },
   "outputs": [],
   "source": [
    "# For the classification portion:\n",
    "model_cls = YOLO('yolov8n-cls.pt')  # Classification model\n",
    "results_cls = model_cls.train(\n",
    "    data=dataset_path,\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    workers=8,\n",
    "    patience=50,\n",
    "    save=True,\n",
    "    project='footprint_classification',\n",
    "    name='yolov8_classification',\n",
    "    pretrained=True,\n",
    "    optimizer='Adam',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3.0,\n",
    "    warmup_momentum=0.8,\n",
    "    warmup_bias_lr=0.1,\n",
    "    verbose=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uzalu2bdgfvN"
   },
   "outputs": [],
   "source": [
    "# Function to ensure dataset has multiple classes for classification\n",
    "def check_and_fix_classification_dataset(dataset_path):\n",
    "    train_path = f\"{dataset_path}/train\"\n",
    "    classes = [d for d in os.listdir(train_path) if os.path.isdir(f\"{train_path}/{d}\")]\n",
    "\n",
    "    print(f\"Found classes: {classes}\")\n",
    "\n",
    "    if len(classes) < 2:\n",
    "        print(f\"Error: Only found {len(classes)} classes. Creating a dummy class for testing.\")\n",
    "        # Create a dummy second class by copying some images from the first class\n",
    "        dummy_class = \"dummy_class\"\n",
    "        os.makedirs(f\"{train_path}/{dummy_class}\", exist_ok=True)\n",
    "\n",
    "        # Copy a few images from the first class\n",
    "        first_class = classes[0]\n",
    "        images = os.listdir(f\"{train_path}/{first_class}\")\n",
    "        for img in images[:min(10, len(images))]:\n",
    "            shutil.copy(f\"{train_path}/{first_class}/{img}\",\n",
    "                        f\"{train_path}/{dummy_class}/{img}\")\n",
    "\n",
    "        print(f\"Created dummy class with {min(10, len(images))} images\")\n",
    "\n",
    "        # Do the same for validation/test set if needed\n",
    "        for split in [\"valid\", \"test\"]:\n",
    "            if os.path.exists(f\"{dataset_path}/{split}\"):\n",
    "                os.makedirs(f\"{dataset_path}/{split}/{dummy_class}\", exist_ok=True)\n",
    "                if os.path.exists(f\"{dataset_path}/{split}/{first_class}\"):\n",
    "                    split_images = os.listdir(f\"{dataset_path}/{split}/{first_class}\")\n",
    "                    for img in split_images[:min(5, len(split_images))]:\n",
    "                        shutil.copy(f\"{dataset_path}/{split}/{first_class}/{img}\",\n",
    "                                    f\"{dataset_path}/{split}/{dummy_class}/{img}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UimujhB5W1m"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "metrics = model.val()\n",
    "print(f\"mAP50-95: {metrics.box.map}\")\n",
    "print(f\"mAP50: {metrics.box.map50}\")\n",
    "print(f\"Precision: {metrics.box.p}\")\n",
    "print(f\"Recall: {metrics.box.r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2E8bOiU5W1m"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.export(format='onnx')  # Export to ONNX format\n",
    "model.save('animal_footprint_detector.pt')  # Save PyTorch model\n",
    "\n",
    "print(\"Training and evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB2vBrWihIJB"
   },
   "source": [
    "------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 2706,
     "status": "error",
     "timestamp": 1742545313767,
     "user": {
      "displayName": "Deva Dethan",
      "userId": "07906698187611358069"
     },
     "user_tz": -330
    },
    "id": "kKpp4QoMhKiG",
    "outputId": "8a4139e2-b176-4443-ecb5-17f6f651172f"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install ultralytics roboflow -q\n",
    "\n",
    "# Import necessary libraries\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Your Roboflow API key\n",
    "api_key = \"\"  # Replace with your actual API key\n",
    "\n",
    "# 1. DOWNLOAD ALL DATASETS IN THEIR CORRECT FORMAT\n",
    "\n",
    "# Function to download dataset from Roboflow in appropriate format\n",
    "def download_dataset(api_key, workspace, project_name, version, format_type=\"yolov8\", location=\"./datasets\"):\n",
    "    rf = Roboflow(api_key=api_key)\n",
    "    project = rf.workspace(workspace).project(project_name)\n",
    "    dataset = project.version(version).download(format_type, location=f\"{location}/{project_name}\")\n",
    "    return dataset, f\"{location}/{project_name}\"\n",
    "\n",
    "# Create directory for downloaded datasets\n",
    "os.makedirs(\"./downloaded_datasets\", exist_ok=True)\n",
    "\n",
    "# Dataset 1: Footprint Classification - Classification dataset\n",
    "footprint_dataset, footprint_path = download_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"sml-project-hfi0w\",\n",
    "    project_name=\"footprint-classification\",\n",
    "    version=1,\n",
    "    format_type=\"folder\",  # Important: use \"folder\" for classification datasets\n",
    "    location=\"./downloaded_datasets\"\n",
    ")\n",
    "\n",
    "# All others are detection datasets - use \"yolov8\" format\n",
    "# Dataset 2: Wild Animal Foot Prints\n",
    "dataset2, dataset2_path = download_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"traffic-bfp0e\",\n",
    "    project_name=\"wild-animal-foot-prints\",\n",
    "    version=1,\n",
    "    format_type=\"yolov8\",\n",
    "    location=\"./downloaded_datasets\"\n",
    ")\n",
    "\n",
    "# Dataset 3: Fred's Test\n",
    "dataset3, dataset3_path = download_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"fredstest\",\n",
    "    project_name=\"othsgdsreq\",\n",
    "    version=38,\n",
    "    format_type=\"yolov8\",\n",
    "    location=\"./downloaded_datasets\"\n",
    ")\n",
    "\n",
    "# Dataset 4: New Animal Detection\n",
    "dataset4, dataset4_path = download_dataset(\n",
    "    api_key=api_key,\n",
    "    workspace=\"lavhini\",\n",
    "    project_name=\"new_animaldetection\",\n",
    "    version=1,\n",
    "    format_type=\"yolov8\",\n",
    "    location=\"./downloaded_datasets\"\n",
    ")\n",
    "\n",
    "print(\"All datasets downloaded successfully!\")\n",
    "\n",
    "# 2. EXAMINE DATASET FORMATS AND EXTRACT CLASS INFORMATION\n",
    "\n",
    "# For classification dataset, get classes from folder structure\n",
    "footprint_classes = []\n",
    "try:\n",
    "    footprint_train_path = os.path.join(footprint_path, \"train\")\n",
    "    if os.path.exists(footprint_train_path):\n",
    "        footprint_classes = [d for d in os.listdir(footprint_train_path)\n",
    "                           if os.path.isdir(os.path.join(footprint_train_path, d))]\n",
    "    print(f\"Footprint classification classes: {footprint_classes}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting footprint classes: {e}\")\n",
    "\n",
    "# For detection datasets, get classes from data.yaml\n",
    "def get_classes_from_yaml(yaml_path):\n",
    "    try:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        return data.get('names', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading YAML file {yaml_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "dataset2_classes = get_classes_from_yaml(f\"{dataset2_path}/data.yaml\")\n",
    "dataset3_classes = get_classes_from_yaml(f\"{dataset3_path}/data.yaml\")\n",
    "dataset4_classes = get_classes_from_yaml(f\"{dataset4_path}/data.yaml\")\n",
    "\n",
    "print(f\"Dataset2 classes (Wild Animal Foot Prints): {dataset2_classes}\")\n",
    "print(f\"Dataset3 classes (Fred's Test): {dataset3_classes}\")\n",
    "print(f\"Dataset4 classes (New Animal Detection): {dataset4_classes}\")\n",
    "\n",
    "# 3. DECIDE ON THE MODEL TYPE: DETECTION is better since most datasets are detection format\n",
    "\n",
    "# Create a unified classes list\n",
    "all_classes = []\n",
    "# Add all class names, avoiding duplicates\n",
    "for class_list in [footprint_classes, dataset2_classes, dataset3_classes, dataset4_classes]:\n",
    "    for cls in class_list:\n",
    "        if cls not in all_classes:\n",
    "            all_classes.append(cls)\n",
    "\n",
    "print(f\"Combined unique classes ({len(all_classes)}): {all_classes}\")\n",
    "\n",
    "# 4. PREPARE COMBINED DATASET DIRECTORY\n",
    "combined_dataset_path = \"./combined_animal_dataset\"\n",
    "os.makedirs(combined_dataset_path, exist_ok=True)\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    os.makedirs(f\"{combined_dataset_path}/{split}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{combined_dataset_path}/{split}/labels\", exist_ok=True)\n",
    "\n",
    "# 5. TRANSFORM CLASSIFICATION DATASET TO DETECTION FORMAT\n",
    "# For the footprint classification dataset, we need to create object detection style annotations\n",
    "\n",
    "def convert_classification_to_detection(image_path, class_name, class_id, output_img_path, output_label_path):\n",
    "    \"\"\"Convert classification image to detection format with full-image bounding box\"\"\"\n",
    "    # Copy the image\n",
    "    shutil.copy(image_path, output_img_path)\n",
    "\n",
    "    # Create a label file with full-image bounding box\n",
    "    with open(output_label_path, 'w') as f:\n",
    "        # Format: class_id x_center y_center width height (normalized 0-1)\n",
    "        f.write(f\"{class_id} 0.5 0.5 1.0 1.0\\n\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# Process footprint classification dataset (convert to detection)\n",
    "# Map class names to indices in the unified class list\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(all_classes)}\n",
    "\n",
    "# Check if footprint dataset exists and has the expected structure\n",
    "if footprint_classes:\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        # Map split names (test/val/valid)\n",
    "        src_split = \"train\" if split == \"train\" else \"valid\" if split == \"valid\" else \"test\"\n",
    "        src_dir = os.path.join(footprint_path, src_split)\n",
    "\n",
    "        if not os.path.exists(src_dir):\n",
    "            print(f\"Split {src_split} not found in footprint dataset\")\n",
    "            continue\n",
    "\n",
    "        # Process each class\n",
    "        for class_name in footprint_classes:\n",
    "            class_dir = os.path.join(src_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                continue\n",
    "\n",
    "            # Get class index in the unified class list\n",
    "            class_id = class_to_idx[class_name]\n",
    "\n",
    "            # Process each image\n",
    "            for img_file in glob.glob(f\"{class_dir}/*.jpg\") + glob.glob(f\"{class_dir}/*.jpeg\") + glob.glob(f\"{class_dir}/*.png\"):\n",
    "                # Create unique filenames\n",
    "                base_name = os.path.basename(img_file)\n",
    "                unique_name = f\"footprint_{class_name}_{base_name}\"\n",
    "\n",
    "                # Output paths\n",
    "                out_img_path = os.path.join(combined_dataset_path, split, \"images\", unique_name)\n",
    "                out_label_path = os.path.join(combined_dataset_path, split, \"labels\",\n",
    "                                             os.path.splitext(unique_name)[0] + \".txt\")\n",
    "\n",
    "                # Convert and copy\n",
    "                convert_classification_to_detection(img_file, class_name, class_id, out_img_path, out_label_path)\n",
    "\n",
    "# 6. COPY AND ADAPT DETECTION DATASETS\n",
    "def process_detection_dataset(dataset_path, dataset_name, class_mapping):\n",
    "    \"\"\"\n",
    "    Process a detection dataset:\n",
    "    1. Copy images to combined dataset\n",
    "    2. Remap class IDs in label files\n",
    "    \"\"\"\n",
    "    # For each split (train/val/test)\n",
    "    for split_src, split_dst in [(\"train\", \"train\"), (\"valid\", \"valid\"), (\"test\", \"test\")]:\n",
    "        images_dir = os.path.join(dataset_path, split_src, \"images\")\n",
    "        labels_dir = os.path.join(dataset_path, split_src, \"labels\")\n",
    "\n",
    "        if not os.path.exists(images_dir) or not os.path.exists(labels_dir):\n",
    "            print(f\"Skipping {dataset_name} {split_src} - directory not found\")\n",
    "            continue\n",
    "\n",
    "        # Process each image\n",
    "        for img_file in glob.glob(f\"{images_dir}/*\"):\n",
    "            img_filename = os.path.basename(img_file)\n",
    "            base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "            # Create unique filename\n",
    "            new_img_filename = f\"{dataset_name}_{img_filename}\"\n",
    "            new_label_filename = f\"{dataset_name}_{base_name}.txt\"\n",
    "\n",
    "            # Copy image\n",
    "            shutil.copy(img_file, os.path.join(combined_dataset_path, split_dst, \"images\", new_img_filename))\n",
    "\n",
    "            # Process label if it exists\n",
    "            label_file = os.path.join(labels_dir, f\"{base_name}.txt\")\n",
    "            if os.path.exists(label_file):\n",
    "                # Read original labels\n",
    "                with open(label_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                # Remap class IDs\n",
    "                new_lines = []\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:  # class_id x y w h\n",
    "                        old_class_id = int(parts[0])\n",
    "                        # Get original class name\n",
    "                        if old_class_id < len(class_mapping):\n",
    "                            class_name = class_mapping[old_class_id]\n",
    "                            # Map to new unified class ID\n",
    "                            new_class_id = class_to_idx.get(class_name)\n",
    "                            if new_class_id is not None:\n",
    "                                parts[0] = str(new_class_id)\n",
    "                                new_lines.append(' '.join(parts) + '\\n')\n",
    "\n",
    "                # Write new label file\n",
    "                with open(os.path.join(combined_dataset_path, split_dst, \"labels\", new_label_filename), 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "\n",
    "# Process each detection dataset\n",
    "if dataset2_classes:\n",
    "    process_detection_dataset(dataset2_path, \"wild_footprints\", dataset2_classes)\n",
    "if dataset3_classes:\n",
    "    process_detection_dataset(dataset3_path, \"freds_test\", dataset3_classes)\n",
    "if dataset4_classes:\n",
    "    process_detection_dataset(dataset4_path, \"new_animal\", dataset4_classes)\n",
    "\n",
    "# 7. CREATE YAML CONFIG FILE FOR THE COMBINED DATASET\n",
    "data_yaml = {\n",
    "    'path': os.path.abspath(combined_dataset_path),\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': len(all_classes),\n",
    "    'names': all_classes\n",
    "}\n",
    "\n",
    "# Write data.yaml\n",
    "with open(os.path.join(combined_dataset_path, 'data.yaml'), 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "\n",
    "# 8. TRAIN THE MODEL WITH PROPER PARAMETERS\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available and set device accordingly\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Use a detection model since we've converted everything to detection format\n",
    "model = YOLO('yolov8n.pt')  # Using the smallest model for testing; use s/m/l/x for better performance\n",
    "\n",
    "# Set training parameters\n",
    "results = model.train(\n",
    "    data=os.path.join(combined_dataset_path, 'data.yaml'),\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16 if torch.cuda.is_available() else 8,  # Smaller batch size for CPU\n",
    "    patience=20,  # Early stopping patience\n",
    "    save_period=10,  # Save checkpoint every 10 epochs\n",
    "    device=device,  # Use GPU if available, otherwise CPU\n",
    "    workers=8 if torch.cuda.is_available() else 2,  # Fewer workers for CPU\n",
    "    pretrained=True,\n",
    "    optimizer='SGD',  # or 'Adam'\n",
    "    lr0=0.01,  # Initial learning rate\n",
    "    lrf=0.01,  # Final learning rate as a fraction of lr0\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3.0,\n",
    "    warmup_momentum=0.8,\n",
    "    warmup_bias_lr=0.1,\n",
    "    box=7.5,\n",
    "    cls=0.5,\n",
    "    dfl=1.5,\n",
    "    val=True,  # Run validation\n",
    "    amp=torch.cuda.is_available(),  # Only use mixed precision with GPU\n",
    "    verbose=True,\n",
    "    project='animal_detection',\n",
    "    name='combined_dataset'\n",
    ")\n",
    "\n",
    "# 9. EVALUATE MODEL\n",
    "metrics = model.val()\n",
    "print(f\"Validation results:\")\n",
    "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"Precision: {metrics.box.p:.4f}\")\n",
    "print(f\"Recall: {metrics.box.r:.4f}\")\n",
    "\n",
    "# 10. SAVE THE MODEL\n",
    "model.export(format='onnx')  # Export to ONNX format\n",
    "print(f\"Model saved to {os.path.join('animal_detection/combined_dataset', model.name)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
